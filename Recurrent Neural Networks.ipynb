{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing an RNN based Language Generation Model\n",
    "__source__ : http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-rnn-with-python-numpy-and-theano/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import theano as T\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize,sent_tokenize,FreqDist\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 20,10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence_start_token = 'SENTENCE_START'\n",
    "sentence_end_token = 'SENTENCE_END'\n",
    "unknown_token = 'UNKNOWN_TOKEN'\n",
    "vocabulary_size= 8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of reddit comments: 65790\n",
      "Size of the corpus: 7531714\n"
     ]
    }
   ],
   "source": [
    "## Opening the dataset file parsing it\n",
    "count,string = 0,''\n",
    "with open('/Users/najeebkhan/Desktop/Dataset/reddit-comments-2015-08.csv') as fp:\n",
    "    for line in fp:\n",
    "        string += line.strip()\n",
    "        count += 1\n",
    "print 'Total number of reddit comments: {}'.format(count)\n",
    "print 'Size of the corpus: {}'.format(len(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "string = string.decode('utf-8')\n",
    "sentences = sent_tokenize(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 62061\n"
     ]
    }
   ],
   "source": [
    "print 'Number of sentences: {}'.format(len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SENTENCE_START body\"I joined a new league this year and they have different scoring rules than I'm used to. SENTENCE_END\n"
     ]
    }
   ],
   "source": [
    "## Converting each of the sentence in the required format\n",
    "sentences = [\"{} {} {}\".format(sentence_start_token, x.encode('utf-8'), sentence_end_token) for x in sentences]\n",
    "print sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62061\n"
     ]
    }
   ],
   "source": [
    "tokenized_sentences = [word_tokenize(sent.decode('utf-8')) for sent in sentences]\n",
    "print len(tokenized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_freq = FreqDist(chain(*tokenized_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words: 88982\n"
     ]
    }
   ],
   "source": [
    "print 'Number of unique words: {}'.format(len(word_freq.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000\n"
     ]
    }
   ],
   "source": [
    "vocabulary = [i[0] for i in word_freq.most_common(vocabulary_size-1)]\n",
    "vocabulary.append(unknown_token)\n",
    "print len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Creating word -> index mapping for each of the words in the vocabulary\n",
    "word_to_index = dict((w,i) for (i,w) in enumerate(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index_to_word = {word_to_index[i]:i for i in word_to_index.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Replacing all of the words not in vocabulary with unknown_token\n",
    "for i in range(len(tokenized_sentences)):\n",
    "    for j in range(len(tokenized_sentences[i])):\n",
    "        if tokenized_sentences[i][j] not in vocabulary:\n",
    "            tokenized_sentences[i][j] = unknown_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (62061,)\n",
      "Training labels: (62061,)\n"
     ]
    }
   ],
   "source": [
    "## Creating the training data set\n",
    "X_train = []\n",
    "y_train = []\n",
    "for sent in tokenized_sentences:\n",
    "    X_train.append([word_to_index[word] for word in sent[:-1]])\n",
    "    y_train.append([word_to_index[word] for word in sent[1:]])\n",
    "X_train = np.asarray(X_train)\n",
    "y_train = np.asarray(y_train)\n",
    "print 'Training set: {}'.format(X_train.shape)\n",
    "print 'Training labels: {}'.format(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String: [u'SENTENCE_START', u'body', u\"''\", u'I', u'joined', u'a', u'new', u'league', u'this', u'year', u'and', u'they', u'have', u'different', u'scoring', u'rules', u'than', u'I', u\"'m\", u'used', u'to', u'.']\n",
      "Word Vector: [1, 524, 6, 8, 3625, 7, 174, 1250, 33, 231, 9, 35, 23, 210, 5154, 381, 88, 8, 63, 216, 5, 2]\n"
     ]
    }
   ],
   "source": [
    "print 'String: {}'.format([index_to_word[i] for i in X_train[0]])\n",
    "print 'Word Vector: {}'.format([i for i in X_train[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: [u'body', u\"''\", u'I', u'joined', u'a', u'new', u'league', u'this', u'year', u'and', u'they', u'have', u'different', u'scoring', u'rules', u'than', u'I', u\"'m\", u'used', u'to', u'.', u'SENTENCE_END']\n",
      "Label Vector: [524, 6, 8, 3625, 7, 174, 1250, 33, 231, 9, 35, 23, 210, 5154, 381, 88, 8, 63, 216, 5, 2, 0]\n"
     ]
    }
   ],
   "source": [
    "print 'Label: {}'.format([index_to_word[i] for i in y_train[0]])\n",
    "print 'Label Vector: {}'.format([i for i in y_train[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class for Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x)/np.sum(np.exp(x),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(x):\n",
    "    temp,i = np.zeros((len(x),vocabulary_size)),0\n",
    "    for j in range(len(x)):\n",
    "        temp[i][x[j]] = 1\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RNN(object):\n",
    "    \n",
    "    def __init__(self,word_dim,hidden_dim=100,bptt_truncate=4):\n",
    "        \n",
    "        ''' Standard python constructor =)'''\n",
    "        \n",
    "        self.word_dim = word_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bptt_truncate = bptt_truncate\n",
    "\n",
    "        ## Initialising the parameters\n",
    "        self.U = np.random.uniform(-1/np.sqrt(word_dim),1/np.sqrt(word_dim),size=(hidden_dim,word_dim))\n",
    "        self.W = np.random.uniform(-1/np.sqrt(hidden_dim),1/np.sqrt(hidden_dim),size=(hidden_dim,hidden_dim))\n",
    "        self.V = np.random.uniform(-1/np.sqrt(hidden_dim),1/np.sqrt(hidden_dim),size=(word_dim,hidden_dim))\n",
    "\n",
    "    def forward_propagation(self,x):\n",
    "        '''\n",
    "            Forward propagation according to\n",
    "            the standard RNN equations. The \n",
    "            function returns both the output\n",
    "            probabilities as well as the weights\n",
    "            infered.\n",
    "        '''\n",
    "        T = len(x)\n",
    "        \n",
    "        ## Initilising the hidden layer and the output layer\n",
    "        s = np.zeros((1+T,self.hidden_dim))\n",
    "        o = np.zeros((T,self.word_dim))\n",
    "        \n",
    "        ## For each time step we calculate the probability fo the next word\n",
    "        for t in range(T):\n",
    "            \n",
    "            s[t] = np.tanh(np.dot(self.U,x[t]) + np.dot(self.W,s[t-1]))\n",
    "            o[t] = softmax(np.dot(self.V,s[t]))\n",
    "        return [o,s]\n",
    "    \n",
    "    def predict(self,x):\n",
    "        o,s = self.forward_propagation(x)\n",
    "        return np.argmax(o,axis=1)\n",
    "    \n",
    "    def calculate_total_loss(self,x,y):\n",
    "        '''\n",
    "            Calculating loss for the whole corpus\n",
    "            The loss is to be summed over all the \n",
    "            sentences in the text\n",
    "        '''\n",
    "        loss,N = 0,0\n",
    "        \n",
    "        for i in range(len(y)):\n",
    "            o,s = self.forward_propagation(x[i])\n",
    "            correct_probs = o[np.arange(len(y[i])),y[i]]\n",
    "            loss += (-1)*np.sum(np.log(correct_probs))\n",
    "            N += len(y[i])\n",
    "        return loss/N\n",
    "    \n",
    "    def bptt(self,x,y):\n",
    "        '''\n",
    "            Backpropagation through time for nudging the\n",
    "            parameters in the right direction\n",
    "        '''\n",
    "        ## Number of timesteps = Length of the sentence into account\n",
    "        T = len(y)\n",
    "        \n",
    "        ## Performing the forward propagation\n",
    "        o,s = self.forward_propagation(x)\n",
    "        \n",
    "        ## Initialising the gradients\n",
    "        dLdU = np.zeros(self.U.shape)\n",
    "        dLdW = np.zeros(self.W.shape)\n",
    "        dLdV = np.zeros(self.V.shape)\n",
    "        \n",
    "        delta_o = o\n",
    "        delta_o[np.arange(len(y)),y] -= 1\n",
    "        \n",
    "        ## Backpropagating in time...\n",
    "        for t in np.arange(T)[::-1]:\n",
    "            \n",
    "            ## Gradient of loss with respect to parameter V\n",
    "            dLdV += np.outer(delta_o[t],s[t].T)\n",
    "            delta_t = np.dot(self.V.T,delta_o[t])*(1 - s[t]**2)\n",
    "            \n",
    "            for bptt_step in np.arange(max(0, t-self.bptt_truncate), t+1)[::-1]:\n",
    "                ## Gradient of loss with respect to W\n",
    "                dLdW += np.outer(delta_t,s[bptt_step - 1])\n",
    "                ## Gradient of loss with respect to U\n",
    "                dLdU[:,x[bptt_step].astype(bool)] += delta_t.reshape(-1,1)\n",
    "                ## Can't understant this o_O\n",
    "                delta_t = self.W.T.dot(delta_t) * (1 - s[bptt_step-1] ** 2)\n",
    "            \n",
    "        return [dLdU,dLdW,dLdV]\n",
    "        \n",
    "    def sgd_step(self,x,y,learning_rate):\n",
    "        '''\n",
    "        Implementing one step of stochastic gradient descent\n",
    "        The implementation is for a sentence of the corpus\n",
    "        '''\n",
    "        dLdU,dLdW,dLdV = self.bptt(x,y)\n",
    "        self.U -= dLdU*learning_rate\n",
    "        self.W -= dLdW*learning_rate\n",
    "        self.V -= dLdV*learning_rate    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def training_sgd(model,X_train,y_train,learning_rate=0.005,nb_epoch=100,evaluate_after_loss=5):\n",
    "    losses = []\n",
    "    nm_examples_seen = 0\n",
    "    \n",
    "    for epoch in range(nb_epoch):\n",
    "        \n",
    "        ## Conditionally printing the loss\n",
    "        if epoch%evaluate_after_loss == 0:\n",
    "            loss = model.calculate_total_loss(X_train,y_train)\n",
    "            losses.append((nm_examples_seen,loss))\n",
    "            print 'Loss after num_examples_seen {} is {}'.format(nm_examples_seen,loss)\n",
    "            \n",
    "            ## Adjusting the learning rate if loss increases\n",
    "            if len(losses) > 1 and losses[-1][1] > losses[-2][1]:\n",
    "                learning_rate *= 0.5\n",
    "                print 'Setting learning rate to {}'.format(learning_rate)\n",
    "        \n",
    "        ## Implementing the SGD step\n",
    "        for i in range(len(y_train)):\n",
    "            model.sgd_step(X_train[i],y_train[i],learning_rate)\n",
    "            nm_examples_seen += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## One hot encoding the training matrix\n",
    "X,y = [],[]\n",
    "for i in range(len(X_train)):\n",
    "    temp1,j = np.zeros((len(X_train[i]),vocabulary_size)),0\n",
    "    temp2,k = np.zeros((len(y_train[i]),vocabulary_size)),0\n",
    "    for word in X_train[i]:\n",
    "        temp1[j][word] = 1\n",
    "        j += 1\n",
    "    X.append(temp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Deleting the old matrix. There should be a function to decode it too! o_O\n",
    "import gc\n",
    "del X_train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Even Fox facing Pro ifs computer artist signing virtual bond interpretation individually many entering emotional Damage hacked priest resulted bearing spit edited\n",
      "Predicted Loss: 8.989\n"
     ]
    }
   ],
   "source": [
    "## Sanity check of the created model!\n",
    "model = RNN(vocabulary_size)\n",
    "probs = model.forward_propagation(X[0])\n",
    "preds = model.predict(X[0])\n",
    "for i in preds:\n",
    "    print index_to_word[i],\n",
    "print\n",
    "print 'Predicted Loss: {}'.format(round(model.calculate_total_loss(X[0:1],y_train[0:1]),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "model = RNN(vocabulary_size)\n",
    "model.sgd_step(X[10], y_train[10], 0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after num_examples_seen 0 is 8.98733893308\n",
      "Loss after num_examples_seen 100 is 8.97512860252\n",
      "Loss after num_examples_seen 200 is 8.95653597722\n",
      "Loss after num_examples_seen 300 is 8.34027526741\n",
      "Loss after num_examples_seen 400 is 6.5010316436\n",
      "Loss after num_examples_seen 500 is 6.14692751803\n",
      "Loss after num_examples_seen 600 is 5.93448051116\n",
      "Loss after num_examples_seen 700 is 5.79655244021\n",
      "Loss after num_examples_seen 800 is 5.67544857506\n",
      "Loss after num_examples_seen 900 is 5.60106649823\n"
     ]
    }
   ],
   "source": [
    "model = RNN(vocabulary_size)\n",
    "losses = training_sgd(model, X[:100], y_train[:100], nb_epoch=10, evaluate_after_loss=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
